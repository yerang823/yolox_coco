{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3521a64d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/coco_yolox/YOLOX'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a2bddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolox import evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a796a5cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3097510/2222471887.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcoco_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./exps/default/yolo_s.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/app/coco_yolox/YOLOX/yolox/evaluators/coco_evaluator.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, model, distributed, half, trt_file, decoder, test_size)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# TODO half to amp_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mtensor_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHalfTensor\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhalf\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataloader import DataLoader as torchDataLoader\n",
    "from yolox.data.dataloading import DataLoader\n",
    "\n",
    "coco_cls=evaluators.COCOEvaluator(img_size=640, dataloader=DataLoader, confthre=0.25,\\\n",
    "                                  nmsthre=0.45, num_classes=62)\n",
    "\n",
    "\n",
    "\n",
    "coco_cls.evaluate(model=)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dfc8c4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|##########| 625/625 [00:48<00:00, 12.97it/s]\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "COCOeval_opt.evaluate() finished in 25.88 seconds.\n",
      "Accumulating evaluation results...\n",
      "COCOeval_opt.accumulate() finished in 4.80 seconds.\n",
      "summaray= Average forward time: 2.81 ms, Average NMS time: 0.92 ms, Average inference time: 3.73 ms\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.239\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.403\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.250\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.132\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.271\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.293\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.232\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.403\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.449\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.260\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.504\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.547\n",
      "per class AP:\n",
      "| class         | AP     | class        | AP     | class          | AP     |\n",
      "|:--------------|:-------|:-------------|:-------|:---------------|:-------|\n",
      "| person        | 41.972 | bicycle      | 17.884 | car            | 28.454 |\n",
      "| motorcycle    | 25.311 | airplane     | 44.954 | bus            | 49.272 |\n",
      "| train         | 41.483 | truck        | 18.923 | boat           | 10.626 |\n",
      "| traffic light | 16.544 | fire hydrant | 47.276 | stop sign      | 50.423 |\n",
      "| parking meter | 28.486 | bench        | 10.656 | bird           | 17.315 |\n",
      "| cat           | 34.784 | dog          | 27.363 | horse          | 35.284 |\n",
      "| sheep         | 29.275 | cow          | 33.498 | elephant       | 46.485 |\n",
      "| bear          | 37.651 | zebra        | 47.014 | giraffe        | 51.935 |\n",
      "| backpack      | 5.715  | umbrella     | 22.199 | handbag        | 4.073  |\n",
      "| tie           | 16.846 | suitcase     | 11.641 | frisbee        | 45.443 |\n",
      "| skis          | 9.949  | snowboard    | 10.042 | sports ball    | 31.713 |\n",
      "| kite          | 29.107 | baseball bat | 13.402 | baseball glove | 24.642 |\n",
      "| skateboard    | 26.284 | surfboard    | 16.780 | tennis racket  | 26.831 |\n",
      "| bottle        | 21.604 | wine glass   | 16.445 | cup            | 24.896 |\n",
      "| fork          | 8.141  | knife        | 3.577  | spoon          | 3.056  |\n",
      "| bowl          | 28.851 | banana       | 10.506 | apple          | 8.436  |\n",
      "| sandwich      | 18.566 | orange       | 18.780 | broccoli       | 14.457 |\n",
      "| carrot        | 10.227 | hot dog      | 12.393 | pizza          | 34.564 |\n",
      "| donut         | 30.018 | cake         | 19.041 | chair          | 15.037 |\n",
      "| couch         | 30.284 | potted plant | 13.309 | bed            | 30.066 |\n",
      "| dining table  | 20.711 | toilet       | 44.051 | tv             | 40.678 |\n",
      "| laptop        | 38.643 | mouse        | 45.020 | remote         | 7.955  |\n",
      "| keyboard      | 32.360 | cell phone   | 18.247 | microwave      | 31.541 |\n",
      "| oven          | 18.885 | toaster      | 15.006 | sink           | 18.663 |\n",
      "| refrigerator  | 26.677 | book         | 7.262  | clock          | 38.952 |\n",
      "| vase          | 16.197 | scissors     | 4.423  | teddy bear     | 25.338 |\n",
      "| hair drier    | 0.000  | toothbrush   | 4.275  |                |        |\n",
      "per class AR:\n",
      "| class         | AR     | class        | AR     | class          | AR     |\n",
      "|:--------------|:-------|:-------------|:-------|:---------------|:-------|\n",
      "| person        | 54.576 | bicycle      | 37.006 | car            | 45.355 |\n",
      "| motorcycle    | 47.466 | airplane     | 64.965 | bus            | 65.194 |\n",
      "| train         | 57.263 | truck        | 51.739 | boat           | 32.948 |\n",
      "| traffic light | 36.688 | fire hydrant | 56.931 | stop sign      | 65.467 |\n",
      "| parking meter | 47.667 | bench        | 32.555 | bird           | 31.944 |\n",
      "| cat           | 60.347 | dog          | 57.018 | horse          | 53.787 |\n",
      "| sheep         | 52.147 | cow          | 52.097 | elephant       | 64.444 |\n",
      "| bear          | 58.451 | zebra        | 62.368 | giraffe        | 63.664 |\n",
      "| backpack      | 29.677 | umbrella     | 44.152 | handbag        | 26.111 |\n",
      "| tie           | 32.421 | suitcase     | 40.736 | frisbee        | 59.913 |\n",
      "| skis          | 31.452 | snowboard    | 29.275 | sports ball    | 43.769 |\n",
      "| kite          | 49.755 | baseball bat | 34.069 | baseball glove | 44.595 |\n",
      "| skateboard    | 46.369 | surfboard    | 38.989 | tennis racket  | 45.822 |\n",
      "| bottle        | 41.826 | wine glass   | 27.713 | cup            | 44.804 |\n",
      "| fork          | 30.233 | knife        | 21.754 | spoon          | 20.158 |\n",
      "| bowl          | 54.767 | banana       | 41.135 | apple          | 35.720 |\n",
      "| sandwich      | 53.672 | orange       | 49.123 | broccoli       | 48.173 |\n",
      "| carrot        | 40.795 | hot dog      | 37.600 | pizza          | 55.986 |\n",
      "| donut         | 55.793 | cake         | 46.161 | chair          | 42.332 |\n",
      "| couch         | 60.881 | potted plant | 40.556 | bed            | 54.785 |\n",
      "| dining table  | 48.360 | toilet       | 62.346 | tv             | 63.542 |\n",
      "| laptop        | 56.017 | mouse        | 58.113 | remote         | 33.074 |\n",
      "| keyboard      | 56.863 | cell phone   | 42.176 | microwave      | 48.727 |\n",
      "| oven          | 46.014 | toaster      | 20.000 | sink           | 44.089 |\n",
      "| refrigerator  | 53.968 | book         | 29.229 | clock          | 55.805 |\n",
      "| vase          | 39.161 | scissors     | 17.778 | teddy bear     | 46.474 |\n",
      "| hair drier    | 0.000  | toothbrush   | 17.544 |                |        |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m yolox.tools.eval -n  yolox-s -c ./YOLOX_outputs/yolox_s/best_ckpt.pth -b 8 -d 1 --conf 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cee0ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2022-02-16 03:05:52.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m256\u001b[0m - \u001b[1mArgs: Namespace(camid=0, ckpt='./YOLOX_outputs/yolox_s/best_ckpt.pth', conf=0.25, demo='image', device='1', exp_file=None, experiment_name='yolox_s', fp16=False, fuse=False, legacy=False, name='yolox-s', nms=0.45, path='../../exp2_yolox/datasets/Cervix_data1/val2017/6645445_2_N2.jpg', save_result=True, trt=False, tsize=640)\u001b[0m\n",
      "\u001b[32m2022-02-16 03:05:53.165\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m266\u001b[0m - \u001b[1mModel Summary: Params: 8.97M, Gflops: 26.81\u001b[0m\n",
      "\u001b[32m2022-02-16 03:05:53.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m279\u001b[0m - \u001b[1mloading checkpoint\u001b[0m\n",
      "\u001b[32m2022-02-16 03:05:54.100\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mloaded checkpoint done.\u001b[0m\n",
      "\u001b[32m2022-02-16 03:05:54.591\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minference\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mInfer time: 0.4369s\u001b[0m\n",
      "\u001b[32m2022-02-16 03:05:54.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_demo\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mSaving detection result in ./YOLOX_outputs/yolox_s/vis_res/2022_02_16_03_05_54/6645445_2_N2.jpg\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python tools/demo.py image -n yolox-s -c ./YOLOX_outputs/yolox_s/best_ckpt.pth --path ../../exp2_yolox/datasets/Cervix_data1/val2017/6645445_2_N2.jpg --conf 0.25 --nms 0.45 --tsize 640 --save_result --device 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33431a39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
